{"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5OAfaF2SX6GfXdZ2Nu8FC"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9007618,"sourceType":"datasetVersion","datasetId":5426450}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport logging\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport sys\nsys.path.append('/kaggle/input/pitch-estimation-files')\nsys.path.append('/opt/conda/lib/python3.10/site-packages')\nfrom conv_blocks import *\nfrom pytorch_layers import *\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\nimport yaml","metadata":{"_uuid":"82f4d790-1de6-4383-8ec7-d7209bad4294","_cell_guid":"7a928552-a76d-4f07-88d1-22dcfdd98ae6","collapsed":false,"id":"4p6sVjhH7EsG","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-22T06:08:44.774903Z","iopub.execute_input":"2024-07-22T06:08:44.775354Z","iopub.status.idle":"2024-07-22T06:08:59.814210Z","shell.execute_reply.started":"2024-07-22T06:08:44.775316Z","shell.execute_reply":"2024-07-22T06:08:59.813041Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2024-07-22 06:08:47.794769: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-22 06:08:47.794921: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-22 06:08:47.948230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"configs = {\n    \"model_params\": {\n        \"in_channels\": 8,\n        \"elayers\": 6,\n        \"hidden_dim\": 768,\n        \"dropout\": 0.2,\n        \"use_ar\": False,\n        \"ar_input\": 512,\n        \"ar_hidden\": 256,\n        \"ar_output\": 128,\n        \"use_tanh\": False,\n        \"dim_feedforward\": 3072,\n        \"use_spk_emb\": False,\n        \"spk_emb_size\": 32,\n        \"spk_emb_hidden\": 32,\n        \"num_ph\": None,\n        \"ph_emb_size\": 8,\n        \"layer_type\": \"default\",\n        \"use_emb\": False,\n        \"num_emb\": 512,\n        \"emb_dim\": 1024,\n        \"emb_p\": None,\n        \"relative_positional_distance\": 100,\n        \"conv_block_type\": \"Original\",\n        \"conv_block_params\": {}\n    }\n}\n\n# Save the configuration dictionary to a YAML file\nyaml_file_path = \"/kaggle/working/configs.yaml\"\nwith open(yaml_file_path, 'w') as file:\n    yaml.dump(configs, file)\n\n#yaml_file_path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CNN + transformer.\n# No heads.\nclass Base_Transformer(nn.Module):\n    def __init__(self, in_channels=8, elayers=6, hidden_dim=768, dropout=.2,\n                    use_ar=False, ar_input=512, ar_hidden=256, ar_output=128, use_tanh=False, dim_feedforward = 3072,\n                    use_spk_emb=False, spk_emb_size=32, spk_emb_hidden=32,\n                    num_ph=None, ph_emb_size=8, layer_type='default',\n                    use_emb=False, num_emb=512, emb_dim=1024, emb_p=None,\n                    relative_positional_distance = 100,\n                    conv_block_type = \"Original\", conv_block_params = {}):\n        super().__init__()\n\n        # Store some params.\n        self.hidden_dim = hidden_dim\n        \n        # Conv block initialization.\n        self.conv_blocks = eval(conv_block_type)\n        self.conv_blocks = self.conv_blocks(**conv_block_params)\n        self.conv_block_type = conv_block_type\n        if conv_block_type == \"Pool\" or conv_block_type == \"Baseline\":\n            self.pool = nn.AvgPool1d(5)\n        self.w_raw_in = nn.Linear(getattr(self.conv_blocks, \"hidden_dim\", hidden_dim), hidden_dim)\n\n        # Transformer layers.\n        if layer_type == 'default':\n            encoder_layer = TransformerEncoderLayer(d_model=hidden_dim, nhead=8, relative_positional=True, \n                                                    relative_positional_distance=relative_positional_distance, \n                                                    dim_feedforward=dim_feedforward, dropout=dropout)\n            logging.info(f\"Using relative positional distance of {relative_positional_distance}\")\n        else:\n            logging.error('layer_type %s not supported' % layer_type)\n            exit()\n        self.transformer = nn.TransformerEncoder(encoder_layer, elayers)\n        \n        \n        # Input embeddings.\n        if num_ph is not None:  # NOTE assuming ph is the input\n            self.in_emb_mat = torch.nn.Embedding(num_ph, ph_emb_size)\n        else:\n            self.in_emb_mat = None\n\n        # Auxilliary features.\n        self.use_ar = use_ar\n        if use_ar:\n            self.ar_model = PastFCEncoder(input_len=ar_input, hidden_dim=ar_hidden, output_dim=ar_output)\n\n        # Speaker embedding.\n        self.use_spk_emb = use_spk_emb\n        if use_spk_emb:\n            self.spk_fc = torch.nn.Linear(spk_emb_size, spk_emb_hidden)\n\n        # Input embeddings.\n        self.use_emb = use_emb\n        if use_emb:\n            if emb_p is None:\n                self.emb_mat = torch.nn.Embedding(num_emb, emb_dim)\n            else:\n                init_array = np.load(emb_p)\n                self.emb_mat = torch.nn.Embedding.from_pretrained(torch.tensor(init_array), freeze=False)\n        \n    def forward(self, x, after_len = None, spk_id=None, spk=None, ar=None, ph=None, **kwargs):\n        \"\"\"\n        Args:\n            x: shape (batchsize, num_in_feats, seq_len).\n            spk: shape (batchsize, spk_emb_dim).\n        \n        Return:\n            out: shape (batchsize, num_out_feats, seq_len).\n        \"\"\"\n        ###### Input Embeddings.\n        if self.use_emb:\n            x = self.emb_mat(x)  # (batchsize, seq_len, emb_dim)\n            x = x.transpose(1, 2)  # (batchsize, emb_dim, seq_len)\n        if self.use_ar:\n            ar_feats = self.ar_model(ar)  # (batchsize, ar_output)\n            ar_feats = ar_feats.unsqueeze(2).repeat(1, 1, x.shape[2])  # (batchsize, ar_output, length)\n            x = torch.cat((x, ar_feats), dim=1)\n        if self.use_spk_emb:\n            cspk = self.spk_fc(spk)\n            cspk = cspk.unsqueeze(2).repeat(1, 1, x.shape[2])\n            x = torch.cat((x, cspk), dim=1)\n        if self.in_emb_mat is not None:\n            # x (batchsize, length)\n            x = self.in_emb_mat(x)  # (batchsize, seq_len, ph_emb_size)\n            x = x.transpose(1, 2)\n        \n        ##### Conv blocks.\n        x = self.conv_blocks(x)\n        x = x.transpose(1, 2)  # (batchsize, seq_len, num_feats)\n        x = self.w_raw_in(x)\n        x = x.transpose(0, 1)  # (seq_len, batchsize, num_feats)\n        \n        # Transformer.\n        # (T, B, C)\n        x = self.transformer(x)\n        \n        \n        # (T, B, C) => (B, C, T)\n        return x.permute(1, 2, 0)\n    \n    def get_output_lengths(self, length):\n        return self.conv_blocks.get_output_lengths(length)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with open('config.yaml', 'r') as file:\n#    config = yaml.safe_load(file)\n\n# Prepare dataset\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\nclass NPYDataset(Dataset):\n    def __init__(self, npy_files, sample_length, add_white_noise=False, noise_level=0.0):\n        self.npy_files = npy_files\n        self.sample_length = sample_length\n        self.add_white_noise = add_white_noise\n        self.noise_level = noise_level\n        self.data = [np.load(file) for file in npy_files]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        \n        if len(sample) < self.sample_length:\n            # Pad or truncate to sample_length if necessary\n            sample = np.pad(sample, (0, self.sample_length - len(sample)), 'constant')\n        else:\n            sample = sample[:self.sample_length]\n        \n        if self.add_white_noise:\n            noise = np.random.normal(0, self.noise_level, sample.shape)\n            sample += noise\n        \n        return torch.tensor(sample, dtype=torch.float32), torch.tensor(sample, dtype=torch.float32)  # Replace with actual target if different\n\nnpy_dir = '/kaggle/input/pitch-estimation-files/dataset/dataset/train'\n\n# Get list of all .npy files in the directory\nnpy_files = glob.glob(os.path.join(npy_dir, '*.npy'))\ndataset = NPYDataset(npy_files, sample_length=256,  \n                     add_white_noise=True, \n                     noise_level=0.01) \ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=None)  \nmodel = CNNTransformerModel(conv_channels=config['model']['conv_channels'], transformer_dim=config['model']['transformer_dim'], transformer_layers=config['model']['transformer_layers'], dropout=config['model']['dropout'])\n\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=config['optimizer']['lr'], weight_decay=config['optimizer']['weight_decay'])\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config['scheduler']['step_size'], gamma=config['scheduler']['gamma'])\nwriter = SummaryWriter(config['logging']['log_dir'])\n\ndef train_model(model, dataloader, criterion, optimizer, scheduler, num_epochs):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, targets in dataloader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        \n        scheduler.step()\n        epoch_loss = running_loss / len(dataloader.dataset)\n        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')\n        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n\nif __name__ == \"__main__\":\n    train_model(model, dataloader, criterion, optimizer, scheduler, config['training']['epochs'])\n    writer.close()","metadata":{},"execution_count":null,"outputs":[]}]}